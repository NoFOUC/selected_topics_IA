{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1  : J'ai choisi de faire la 2ème méthode "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work With mps for MacOS M1 Pro and Max \n",
    "also with cuda but not tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-13.1-arm64-arm-64bit'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.6\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (23.1)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/protobuf-4.21.12-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "\n",
    "!python3 --version\n",
    "!pip3 install torch\n",
    "!python3.11 -m pip install --upgrade pip\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if MPS is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    gpu = torch.device(\"mps\")\n",
    "    x = torch.ones(5, device=gpu)\n",
    "    mps_active = True\n",
    "    print (x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data + encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-18 19:55:25--  https://www.gutenberg.org/files/11/11-0.txt\n",
      "Résolution de www.gutenberg.org (www.gutenberg.org)… 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connexion à www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 174313 (170K) [text/plain]\n",
      "Sauvegarde en : « 11-0.txt »\n",
      "\n",
      "11-0.txt            100%[===================>] 170,23K   680KB/s    ds 0,3s    \n",
      "\n",
      "2023-04-18 19:55:27 (680 KB/s) — « 11-0.txt » sauvegardé [174313/174313]\n",
      "\n",
      "--2023-04-18 19:55:27--  https://www.gutenberg.org/files/66927/66927-0.txt\n",
      "Résolution de www.gutenberg.org (www.gutenberg.org)… 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connexion à www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 532433 (520K) [text/plain]\n",
      "Sauvegarde en : « 66927-0.txt »\n",
      "\n",
      "66927-0.txt         100%[===================>] 519,95K   125KB/s    ds 4,2s    \n",
      "\n",
      "2023-04-18 19:55:33 (125 KB/s) — « 66927-0.txt » sauvegardé [532433/532433]\n",
      "\n",
      "--2023-04-18 19:55:33--  https://www.gutenberg.org/cache/epub/18092/pg18092.txt\n",
      "Résolution de www.gutenberg.org (www.gutenberg.org)… 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connexion à www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 435335 (425K) [text/plain]\n",
      "Sauvegarde en : « pg18092.txt »\n",
      "\n",
      "pg18092.txt         100%[===================>] 425,13K   226KB/s    ds 1,9s    \n",
      "\n",
      "2023-04-18 19:55:40 (226 KB/s) — « pg18092.txt » sauvegardé [435335/435335]\n",
      "\n",
      "﻿The Project Gutenberg EBook of Germaine, by Edmond About\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Germaine\n",
      "\n",
      "Author: Edmond About\n",
      "\n",
      "Release Date: April 1, 2006 [EBook #18092]\n",
      "\n",
      "[Date last updated: April 10, 2006]\n",
      "\n",
      "Language: French\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK GERMAINE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Carlo Traverso, Renald Levesque and the Online\n",
      "Distributed Proofreading Team at http://www.pgdp.net (This\n",
      "file was produced from images generously made available\n",
      "by the Bibliothèque nationale de France (BnF/Gallica))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                               GERMAINE\n",
      "\n",
      "                                  PAR\n",
      "\n",
      "                             EDMOND ABOUT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                         SOIXANTE-SIXIÈME MILLE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  PARIS\n",
      "                        LIBRAIRIE HACH\n",
      "test\n",
      "[ 44  24  93  30   3  57  92  27  87  30  51  85   3  59   0  85  30  33\n",
      "  13  30  92  98   3   7  74  27  27  79   3  27  46   3  59  30  92 100\n",
      "  21  83  33  30  19   3  13  77   3   7  40 100  27  33  40   3  10  13\n",
      "  27   0  85  18  18  24  93  83  25   3  30  74  27  27  79   3  83  25\n",
      "   3  46  27  92   3  85  93  30   3   0  25  30   3  27  46   3  21  33\n",
      "  77  27  33  30   3  21  33  77  26  93  30  92  30   3  21  85   3  33\n",
      "  27   3  51  27  25  85   3  21  33  40   3  26  83  85  93  18  21  35\n",
      " 100  27  25  85   3  33  27   3  92  30  25  85  92  83  51  85  83  27\n",
      "  33  25   3  26  93  21  85  25  27  30  53  30  92  31   3   3   9  27\n",
      "   0   3 100  21  77   3  51  27  47  77   3  83  85  19   3  98  83  53\n",
      "  30   3  83  85   3  21  26  21  77   3  27  92  18  92  30  90   0  25\n",
      "  30   3  83  85   3   0  33  40  30  92   3  85  93  30   3  85  30  92\n",
      " 100  25   3  27  46   3  85  93  30   3  57  92  27  87  30  51  85   3\n",
      "  59   0  85  30  33  13  30  92  98   3   5  83  51  30  33  25  30   3\n",
      "  83  33  51  35   0  40  30  40  18  26  83  85  93   3  85  93  83  25\n",
      "   3  30  74  27  27  79   3  27  92   3  27  33  35  83  33  30   3  21\n",
      "  85   3  26  26  26  31  98   0  85  30  33  13  30  92  98  31  27  92\n",
      "  98  18  18  18  24  83  85  35  30  48   3  59  30  92 100  21  83  33\n",
      "  30  18  18  10   0  85  93  27  92  48   3   7  40 100  27  33  40   3\n",
      "  10  13  27   0  85  18  18  17  30  35  30  21  25  30   3  70  21  85\n",
      "  30  48   3  10  47  92  83  35   3  96  19   3  63  37  37  69   3  82\n",
      "   7  74  27  27  79   3  80  96  34  37   2  63  28  18  18  82  70  21\n",
      "  85  30   3  35  21  25  85   3   0  47  40  21  85  30  40  48   3  10\n",
      "  47  92  83  35   3  96  37  19   3  63  37  37  69  28  18  18   5  21\n",
      "  33  98   0  21  98  30  48   3  38  92  30  33  51  93  18  18  18  60\n",
      "  60  60   3  41  24  10  17  24   3  68  38   3  24   8   4  41   3  57\n",
      "  17  68  20   7   6  24   3  59  11  24   7  52  74   7  17  59   3   7\n",
      "  74  68  68  61   3  59   7  17  94  10   4  52   7   3  60  60  60  18\n",
      "  18  18  18  18  57  92  27  40   0  51  30  40   3  13  77   3   6  21\n",
      "  92  35  27   3  24  92  21  53  30  92  25  27  19   3  17  30  33  21\n",
      "  35  40   3   5  30  53  30  25  56   0  30   3  21  33  40   3  85  93\n",
      "  30   3  68  33  35  83  33  30  18  70  83  25  85  92  83  13   0  85\n",
      "  30  40   3  57  92  27  27  46  92  30  21  40  83  33  98   3  24  30\n",
      "  21 100   3  21  85   3  93  85  85  47  48  84  84  26  26  26  31  47\n",
      "  98  40  47  31  33  30  85   3  43  24  93  83  25  18  46  83  35  30\n",
      "   3  26  21  25   3  47  92  27  40   0  51  30  40   3  46  92  27 100\n",
      "   3  83 100  21  98  30  25   3  98  30  33  30  92  27   0  25  35  77\n",
      "   3 100  21  40  30   3  21  53  21  83  35  21  13  35  30  18  13  77\n",
      "   3  85  93  30   3  74  83  13  35  83  27  85  93  14  56   0  30   3\n",
      "  33  21  85  83  27  33  21  35  30   3  40  30   3  38  92  21  33  51\n",
      "  30   3  43  74  33  38  84  59  21  35  35  83  51  21  88  88  18  18\n",
      "  18  18  18  18  18  18   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3  59   7  17  94  10   4  52   7  18  18   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3  57  10  17  18  18   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   7  70  94  68  52  70   3  10  74  68  11\n",
      "  24  18  18  18  18  18   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3  41  68   4  42  10\n",
      "  52  24   7  90  41   4  42   4  29  94   7   3  94   4   5   5   7  18\n",
      "  18  18  18  18   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3  57  10  17   4  41  18   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   5   4  74  17\n",
      "  10   4  17   4   7   3   8  10   6   8]\n"
     ]
    }
   ],
   "source": [
    "# check if the gpu is available\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# check if mps acceleration is available\n",
    "\n",
    "train_with_mps = torch.has_mps\n",
    "\n",
    "# import the data\n",
    "# the data is a text file with a lot of words of the english language\n",
    "# download the data from https://www.gutenberg.org/files/11/11-0.txt\n",
    "\n",
    "!wget https://www.gutenberg.org/files/11/11-0.txt\n",
    "\n",
    "!wget https://www.gutenberg.org/files/66927/66927-0.txt\n",
    "\n",
    "!wget https://www.gutenberg.org/cache/epub/18092/pg18092.txt\n",
    "\n",
    "\n",
    "# read the data\n",
    "\n",
    "with open('pg18092.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print the first 1000 characters of the data\n",
    "\n",
    "print(text[:1000])\n",
    "\n",
    "print(\"test\")\n",
    "# create a dictionary of the characters in the data\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "# encode the text\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "\n",
    "\n",
    "# print the first 1000 characters of the encoded data\n",
    "\n",
    "print(encoded[:1000])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to get the batches of the data\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "\n",
    "    arr = arr[:batch_size * n_batches]\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class to create the model\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                 drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    # define function \"to\" to send to gpu or mps device if available\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super().to(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "     \n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to one hot encode the data\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the train with verification of cpu, mps or cuda run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to train the model\n",
    "\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    if (torch.backends.mps.is_available()):\n",
    "        net.to(gpu)\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            if (torch.backends.mps.is_available()):\n",
    "                inputs = inputs.to(gpu)\n",
    "                targets = targets.to(gpu)\n",
    "                h = tuple([each.data.to(gpu) for each in h])\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                h = tuple([each.data.cuda() for each in h])\n",
    "            \n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "\n",
    "                    if (torch.backends.mps.is_available()):\n",
    "                        inputs = inputs.to(gpu)\n",
    "                        targets = targets.to(gpu)\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to predict the next character\n",
    "\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if (torch.backends.mps.is_available()):\n",
    "        inputs = inputs.to(gpu)\n",
    "        h = tuple([each.to(gpu) for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "        h = tuple([each.cuda() for each in h])\n",
    "    \n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.cpu().numpy().squeeze()\n",
    "\n",
    "    p = p.cpu().numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    return net.int2char[char], h\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(104, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=104, bias=True)\n",
      ")\n",
      "Epoch: 1/20... Step: 10... Loss: 3.2122... Val Loss: 3.3380\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1260... Val Loss: 3.3408\n",
      "Epoch: 2/20... Step: 30... Loss: 3.1565... Val Loss: 3.3262\n",
      "Epoch: 2/20... Step: 40... Loss: 3.1237... Val Loss: 3.3208\n",
      "Epoch: 2/20... Step: 50... Loss: 3.1118... Val Loss: 3.3167\n",
      "Epoch: 3/20... Step: 60... Loss: 3.0887... Val Loss: 3.3050\n",
      "Epoch: 3/20... Step: 70... Loss: 3.0779... Val Loss: 3.2695\n",
      "Epoch: 3/20... Step: 80... Loss: 3.0329... Val Loss: 3.2563\n",
      "Epoch: 4/20... Step: 90... Loss: 2.9155... Val Loss: 3.1979\n",
      "Epoch: 4/20... Step: 100... Loss: 2.8494... Val Loss: 3.1675\n",
      "Epoch: 4/20... Step: 110... Loss: 2.7539... Val Loss: 3.1061\n",
      "Epoch: 5/20... Step: 120... Loss: 2.7069... Val Loss: 3.0564\n",
      "Epoch: 5/20... Step: 130... Loss: 2.5734... Val Loss: 2.9710\n",
      "Epoch: 5/20... Step: 140... Loss: 2.5604... Val Loss: 2.9678\n",
      "Epoch: 6/20... Step: 150... Loss: 2.4640... Val Loss: 2.9962\n",
      "Epoch: 6/20... Step: 160... Loss: 2.3740... Val Loss: 2.8595\n",
      "Epoch: 6/20... Step: 170... Loss: 2.3522... Val Loss: 2.8321\n",
      "Epoch: 7/20... Step: 180... Loss: 2.3189... Val Loss: 2.7742\n",
      "Epoch: 7/20... Step: 190... Loss: 2.2522... Val Loss: 2.7623\n",
      "Epoch: 7/20... Step: 200... Loss: 2.2091... Val Loss: 2.7259\n",
      "Epoch: 8/20... Step: 210... Loss: 2.2048... Val Loss: 2.6860\n",
      "Epoch: 8/20... Step: 220... Loss: 2.1447... Val Loss: 2.6704\n",
      "Epoch: 8/20... Step: 230... Loss: 2.1025... Val Loss: 2.6596\n",
      "Epoch: 9/20... Step: 240... Loss: 2.0587... Val Loss: 2.6453\n",
      "Epoch: 9/20... Step: 250... Loss: 2.0462... Val Loss: 2.6267\n",
      "Epoch: 9/20... Step: 260... Loss: 2.0091... Val Loss: 2.6078\n",
      "Epoch: 10/20... Step: 270... Loss: 1.9844... Val Loss: 2.5935\n",
      "Epoch: 10/20... Step: 280... Loss: 1.9470... Val Loss: 2.5719\n",
      "Epoch: 10/20... Step: 290... Loss: 1.9276... Val Loss: 2.5710\n",
      "Epoch: 11/20... Step: 300... Loss: 1.8680... Val Loss: 2.5592\n",
      "Epoch: 11/20... Step: 310... Loss: 1.8666... Val Loss: 2.5384\n",
      "Epoch: 12/20... Step: 320... Loss: 1.9149... Val Loss: 2.5335\n",
      "Epoch: 12/20... Step: 330... Loss: 1.8316... Val Loss: 2.5165\n",
      "Epoch: 12/20... Step: 340... Loss: 1.8195... Val Loss: 2.5060\n",
      "Epoch: 13/20... Step: 350... Loss: 1.8070... Val Loss: 2.4969\n",
      "Epoch: 13/20... Step: 360... Loss: 1.7808... Val Loss: 2.4814\n",
      "Epoch: 13/20... Step: 370... Loss: 1.7601... Val Loss: 2.4688\n",
      "Epoch: 14/20... Step: 380... Loss: 1.7661... Val Loss: 2.4512\n",
      "Epoch: 14/20... Step: 390... Loss: 1.6822... Val Loss: 2.4561\n",
      "Epoch: 14/20... Step: 400... Loss: 1.7016... Val Loss: 2.4540\n",
      "Epoch: 15/20... Step: 410... Loss: 1.7253... Val Loss: 2.4404\n",
      "Epoch: 15/20... Step: 420... Loss: 1.6732... Val Loss: 2.4376\n",
      "Epoch: 15/20... Step: 430... Loss: 1.6698... Val Loss: 2.4330\n",
      "Epoch: 16/20... Step: 440... Loss: 1.6676... Val Loss: 2.4331\n",
      "Epoch: 16/20... Step: 450... Loss: 1.6468... Val Loss: 2.4214\n",
      "Epoch: 16/20... Step: 460... Loss: 1.6269... Val Loss: 2.4146\n",
      "Epoch: 17/20... Step: 470... Loss: 1.6488... Val Loss: 2.4100\n",
      "Epoch: 17/20... Step: 480... Loss: 1.5997... Val Loss: 2.4020\n",
      "Epoch: 17/20... Step: 490... Loss: 1.5705... Val Loss: 2.3999\n",
      "Epoch: 18/20... Step: 500... Loss: 1.5972... Val Loss: 2.3841\n",
      "Epoch: 18/20... Step: 510... Loss: 1.5749... Val Loss: 2.3912\n",
      "Epoch: 18/20... Step: 520... Loss: 1.5431... Val Loss: 2.3932\n",
      "Epoch: 19/20... Step: 530... Loss: 1.5234... Val Loss: 2.3719\n",
      "Epoch: 19/20... Step: 540... Loss: 1.5397... Val Loss: 2.3813\n",
      "Epoch: 19/20... Step: 550... Loss: 1.5224... Val Loss: 2.3791\n",
      "Epoch: 20/20... Step: 560... Loss: 1.5113... Val Loss: 2.3572\n",
      "Epoch: 20/20... Step: 570... Loss: 1.4846... Val Loss: 2.3780\n",
      "Epoch: 20/20... Step: 580... Loss: 1.4920... Val Loss: 2.3791\n"
     ]
    }
   ],
   "source": [
    "# launch the model training\n",
    "\n",
    "n_hidden=1024\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "\n",
    "\n",
    "net.to(gpu)\n",
    "\n",
    "\n",
    "print(net)\n",
    "\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appartement\n",
      "docteur\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# define the function to generate the text\n",
    "\n",
    "def sample(net, size, prime='L', top_k=None):\n",
    "\n",
    "    x = np.array([[net.char2int[ch] for ch in prime]])\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    if (torch.backends.mps.is_available()):\n",
    "        net.to(gpu)\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    chars = [ch for ch in prime]\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# generate the text\n",
    "\n",
    "a = sample(net, 3, prime='appartem', top_k=5)\n",
    "print(a)\n",
    "\n",
    "# save the model\n",
    "\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "                'n_layers': net.n_layers,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "\n",
    "# load the model\n",
    "\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# print the generated text\n",
    "\n",
    "b= sample(loaded, 1, top_k=5, prime=\"docteu\")\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.349862745999265\n"
     ]
    }
   ],
   "source": [
    "#calculate the perplexity\n",
    "\n",
    "def perplexity(net, data, seq_length=100):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    n_chars = len(net.chars)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    h = net.init_hidden(1)\n",
    "    val_losses = []\n",
    "    for x, y in get_batches(data, 1, seq_length):\n",
    "        x = one_hot_encode(x, n_chars)\n",
    "        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        inputs, targets = x, y\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        if (torch.backends.mps.is_available()):\n",
    "            inputs = inputs.to(gpu)\n",
    "            targets = targets.to(gpu)\n",
    "            \n",
    "\n",
    "        output, h = net(inputs, h)\n",
    "        val_loss = criterion(output, targets.view(1*seq_length).long())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    return np.exp(np.mean(val_losses))\n",
    "\n",
    "print(perplexity(loaded, encoded, seq_length=100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## Comment pourriez-vous utiliser TF-IDF également pour simplifier ou améliorer vos modèles?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la première methode je pense que TF-IDF pourrait reduire le nombre de caractères de la fenêtre d'entrée car avec les poids pour chaque caractère calculer à partir de TF-IDF, on peut identifier les caractère qui ont le plus de poids pour les caractère suivants.\n",
    "On reduit la dimensionnalité en se concentrant sur ces caractère uniquement.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la deuxième, je pense que ça ne peut pas améliorer le modèle enormément mais on pourrait améliorer la perplexité. \n",
    "on pourrait utiliser TF-IDF pour filtrer les caractères peu fréquents de la même manière que pour la premère méthode "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
